Part 2 :

Regularization with Linear and Kernel Least Squares

	Remark: The curse of dimensionality
	 - Q: What is the length of the edge of a cube containing 1% of the volume
	      of a cube with edge length 1?
	   A: If the dimension is 3 (what we traditionally think of) then the edge
	   length is (0.01)^(1/3). But this isn't necessarily true. A cube can 
	   have k dimensions (k>0). For each case, the edge length of the smaller
	   cube changes! This is the curse of dimensionality, as things get 
	   larger, things tend to change. 

	Minimization of Sum of squares

	Consider the following algorithm

	(A)
	min {w in R^D} 1/n * sum {i=1,..,n} (y_i - w^T*x_i)^2 + lambda*w^T*w 
		for lambda >= 0
		where f(x) = w^T*x = 0
	* Methods like this are called tikhonov regularization

	Note that 1/n * sum (yi - wt*xi)^2 = 1/n * ||Y_n - X_n*w||^2

	Taking the partial derivatives of (A) we receive
		- 2/n * X_n^T(Y_n - X_n*w)
		2w

	Setting these Equal to zero (I.e solving for the point at which these are zero)
	which if you remember from calculus will solve the function, we get 

	(X_n^T*X_n + lambda*n*I)*w = X_n^T*Y_n

Interlude: Intro to Least Squares with David Bindel
	(Notes from http://www.cs.cornell.edu/courses/cs4220/2017sp/lec/2017-02-22.pdf)
	 Least sqaures are special minimization problems because generally we cannot
	 solve the overdetermined system. Assume A in R^{m x n} with m > n. For this 
	 generall the best we can do is minimize the reisual r = b-A*x. In least squares
	 we solve the problem argmin_x ||r||^2. 

	 * Note this is not the only way to solve the system 

	Example: Cricket Chrips
	 Want to estimate the temperature by listening to the rate of chirps.
	 	temp = a*chirps + beta + eps where eps is an error term 
	 To solve by linear regression minimize the residual 
	 		r = b-A*x 
	 		where b_i = temp in experiment i
	 			  A_i1 = chirps in experiment i
	 			  A_i2 = 1
	 			  x = [alpha ; beta]
	 Matlab and octave can solve this easily using the backslash operator
	  A = [chirps, ones(ndata,1)];
	  x = A \ temp;

	 However most of the time, we have more than two variables we want to fit.

	 This is where the normal equations come in. When we minimize the euc norm 
	 of r = b-A*x we find that r is normal to everything in the range space of
	 A. or in other terms that 
	 	for all z in R^n 
	 		0 = (A*z)^T(b-A*x) = z^T*(A^T*b - A^T*A*x)
	 This leads us to the normal equations A^TAx = A^Tb 

	 There are a variety of ways one can now solve this. One is the cholesky 
	 factorization is A is full rank. This => A^T*A is SPD and we can
	 compute a cholesky factorization of A^T*A = R^T*R . Then the solution
	 to LS is x = (A^T*A)^{-1}*A^T*b = R^{-1}*R^{-T}*A^{T}*b
	 or in matlab 
	 	R = chol(A'*A, 'upper');
	 	x = R\(R'\(A'*b));

 Regularization with Bindel 
	 (Notes from http://www.cs.cornell.edu/courses/cs4220/2017sp/lec/2017-02-27.pdf)
	Say you want to compute the perimeter of a square but have forgotten basic
	geometry. You set up the following model
		perimeter = alph*side len + beta*diagnol
	and from that set up a least squares system
		A*x = b
		with A = [s sqrt(2)*s]
	 		 b = 4s
	where s is a vector of side lengths. Then the normal equations
	can be found to be 
	 	A^T*A = ||s||^2 * [1 sqrt(2); sqrt(2) 2]
	 	A^T*b = ||s||^2 * [4 ; 4*sqrt(2)]
	This system is ill-posed, i.e it has more than one solution, and the
	equations are singular. Naive linear regression is likely to run into
	som trouble when attempting to solve this. This is where our boy Tikhonov
	and more come in with regularization. 	

 	Bias-Variance Tradeoff

 	Least squares is often used to fit a model used for prediction in the future. 

 	Def: Bias-Variance - Prediction error consists of a bias term due to using a 
 	space of models that doesn't actually fit the data, and a term related to
 	the variance as a function of measurement noise

 	Bindel in his notes does a lot of cool analysis on bounding the error in the
 	online notes to definetly check that out!
 	The main idea is that you want to balance both terms (the bias and the variance)
 	when coming to a solution.

 	To solve this consider 4 methods:
 		1. Factor selection via pivoted QR
 		2. Tikhonov Regularization 
 		3. Truncated SVD regularization
 		4. l1 norm regularization (Also called lasso)

 	In these notes I will specifically talk about Tikhonov since it is pertinent 
 	to the online course. 

 	Consider the tikhonov least squares problem
 		minimize ||Ax-b||^2 + lambda^2*||x||^2 
 	which can be also written as 
 		minimize || [A; lambda*I]x - [b; 0] ||^2
 	This leads to a regulatized version of the normal equations
 		(A^T*A + lambda^2*I)*x = A^T*b
 	
 	In some cases we may want to regularize with a more general norm 
 		||x||^2_M = x^T*M*x where M is SPD
 	This leads to  
 		(A^T*A + lambda^2*M)*x = A^T*b

 	If we don't know of any structure beforehand M=I is a good choice.
 	Its useful to compare the usual lsq solution to the regularized
 	solution via the SVD. If A=USV^T is the economy SVD, then
 		x_LS = V * S^{-1} * U^T * b
 		x_tik = V * f(S)^{-1} * U^T * b
 		where f(sigma)^{-1} = sigma / (sigma^2 + lambda^2)
 	This function is a filter of the inverse singular values, which affects
 	large values only a little bit but dampense small singular values. 

 	An alternative to Tikhonov regularization is the l1 norm problem also
 	called the "lasso" approach. 
 		minimize ||Ax-b||^2 + lambda*||x||_1
 	The problem has the property that solutions tend to become sparse
 	as lambda becomes larger. 
 	The lasso can be solved via convex optimization techniques.
 Back to class
 If M is SPD then an eigendecompesition can be found 
 	M = VSV^T where S=diag(t1,t2,...,td) (i.e the singular values), VV^T=I
 	then M^{-1} = VS^{-1}V^{-1} where S^{-1} = diag(1/t1,...,1/td)
 	and finally (M+lambda*I)^{-1} = VS_lV^{T} where S_l = diag(1/[t1+lambda],...,1/[td+lambda])
 Now look back at the originarl problem
 min {w in R^D} 1/n * sum {i=1,..,n} (y_i - w^T*x_i)^2 + lambda*w^T*w 
		for lambda >= 0
		where f(x) = w^T*x = sum_j wj*xj
 Note that f(x) has changed here.
 We can now start to talk about kernels.

	 (Notes from CS 4780 http://www.cs.cornell.edu/courses/cs4780/2017sp/lectures/lecturenote13.html)
	 We can make classifiers non-linear by applying a basis function on input.
	 For a data vector x in R^d do a transformation x -> phi(x) I will denote this px from now on,
	 where px in R^D, with usually D>>d. The advantage of this is that its simple, problem remains 
	 convex, and the well behaved, but the disadvantage is that it might become very high dimensional.

	 Grad Descent with squared loss. l(w) = sum {i=1,..,n} (w^T*x_i - y_i)^2 . The gradient descent rule s>0 
	 updates w over time. It can be showed that w = sum {i=1,..,n} (a_i*x_i)
	 We can also express w^T*x_j = sum {i=1,..,n} a_i*x_i^T*x_j leading to us rewriting the squared loss
	 in terms of inner products l(a) = sum {i=1,..,n} [ sum {j=1,..,n} (a_j*x_j^T*x_i - y_i)^2 ].
	 During testing one only needs these coefficiants to make a prediction on a test-input x_t 
	 and thus we can write the classifier in terms of inner products h(x_t) = w^T*x_t = sum {j=1,..,n} a_j*x_j^T*x_t

	 Going back to px, we can show that the inner product px^T*px = pi {k=1,..,d} (1+x_k*z_k) (Pi here is the multiplier
	 i.e instead of summing you multiply). So the sum of 2^d terms becomes the product of d terms reducing computation 
	 to O(d) from O(2^d)! The main point here is that you can predompute these values and store them in a kernel matrix
	 K(x_i,x_j) = px_i^T*px_j . By storing K, we only need to do simple inner product lookups and low-dimensional
	 computations throughout the general gradient descent algorithm. 

	 Thereare many different kernel functions that you can use.
Back to class
	using px then f(x) = w^T*px = sum {j=1,..,p} p_j(x)*w^j
	Then (X_n^T*X_n + lambda*n*I)*w = X_n^T*Y_n becomes (pX_n^T*pX_n+lambda*n*Y)*w = pX_n^T*Y_n
	Then we can solve for w to be
		w = X_n^T*(X_n*X_n^T+lambda*n*I)^{-1}*Y_n = sum {i=1,..,n} x_i^T*c_i (c_i is just them being lazy and not writing
		the whole thing out lol)
	The kernel w = sum {i=1,..,n} x_i*c_i implies that f(x)=x^T*w = sum {j=1,..,n} x^T*x_i*c_i where we can say K(x,x_i) = x^T*x_i
	Thus (K_n + lambda*n*I)^{-1}*c = Y_n, where (K_n)_i,j = K(x_j,x_j)

	The linear kernel is K(x,x') = x^T*x'
	The polynomial kernel is K(x,x') = (x^T*x' + 1)^d
	The gaussian kernel is K(x,x') = exp[-||x-x'||^2 / 2*sigma^2]

End of Part 2